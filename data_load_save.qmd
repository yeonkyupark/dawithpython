# 데이터 적재와 저장

데이터 분석 첫 단계인 데이터 적재와 분석이 완료된 정보를 저장하는 방법을 알아 본다.

## 데이터 적재 

### CSV 파일 적재

데이터를 적재하는 가장 기본적인 방법은 `pandas.read_csv()` 함수를 사용하는 것이다.

```python
pandas.read_csv(filepath_or_buffer, 
                sep='구분자', 
                header="컬럼명이 있는 행번호", 
                encoding="인코딩종류", 
                na_values=["결측치로 인식할 값"], 
                keep_default_na=Fasle)
```

`keep_default_na` 값을 `True`로 설정하면 기본 결측치 목록(NaN, N/A, NA, NULL, None, 빈문자열)을 그대로 사용한다. 반면 `False`로 지정하면 `na_values` 목록만 결측치로 변환한다. `False` 지정 후 `na_values`를 설정하지 않으면 어떤 값도 자동으로 결측치로 변환되지 않는다.

```{python}
import pandas as pd

url = "https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/\
palmerpenguins/data/penguins.csv"

dataset = pd.read_csv(url)

print(dataset.head())
```


`header` 위치를 변경하면 다음과 같은 결과를 얻는다.


```{python}
dataset = pd.read_csv(url, header = 1)

print(dataset.head())
```


원본 데이터에는 결측치를`NA`로 처리했다. `keep_default_na` 옵션 변경에 따라 결측치는 다르게 처리된다.


```{python}
dataset = pd.read_csv(url, keep_default_na=False)

print(dataset.head())
```


## 데이터 저장 

처리된 데이터는 상황이나 과제에 따라 다양한 형태로 저장할 수 있다. 우선 가장 일반적인 csv 형태로 저장하는 방법을 알아 본다.

### CSV 파일 저장

```python
DataFrame.to_csv('저장할 경로',
                encoding='인코딩지정',
                index='인덱스포함여부',
                sep='구분자')
```

데이터 저장 시 어떤 인코딩을 적용했는지 명시적으로 지정한다. pandas 경우 `utf-8`을 기본 인코딩으로 사용한다. 반면 윈도우즈 엑셀은 `cp949`를 인코딩으로 사용한다. 이렇게 인코딩이 다른 경우 한글이 깨지는 현상이 발생한다.

```{python}
import pandas as pd

# 한글이 포함된 DataFrame 생성
df = pd.DataFrame({
    "이름": ["홍길동", "김철수"],
    "부서": ["생산관리", "스마트팩토리"]
})

# UTF-8로 저장 (BOM 없음)
df.to_csv("korean_utf8.csv", index=False, encoding="utf-8")
```


```{python}
df_broken = pd.read_csv(
    "korean_utf8.csv",
    encoding="cp949",
    encoding_errors="replace"
)

print(df_broken)
```


위 예제는 `utf-8`로 저장하고 `cp949`로 읽었을 때 상황이다. 인코딩을 맞추게 되면 정상 출력된다.


```{python}
df_matched = pd.read_csv(
    "korean_utf8.csv",
    encoding="utf-8",
    encoding_errors="replace"
)

print(df_matched)
```

## 결측치 처리 

결측치는 누락된 값을 의미한다. 즉 데이터를 정상적으로 입력하지 못한 경우이다.

이런 결측치는 발생 원인과 특성에 따라 크게 3가지로 구분된다.

- **완전 무작위 결측** (MCAR, Missing Completely At Random)
    - 결측 발생이 어떤 변수와도 관련이 없는 경우
    - 예시: 설문 데이터 입력 중 시스템 오류로 임의의 몇 행이 통째로 누락됨, 사람이 데이터 입력 시 실수로 데이터를 누락하는 경우
- **무작위 결측** (MAR, Missing At Random)
    - 결측 여부가 다른 관측된 변수와는 관련 있지만, 자기 자신 값과는 직접적 관련이 없는 경우
    - 예시: 고연령층 응답자일수록 소득 항목을 응답하지 않는 경우 (연령은 존재, 소득만 결측), 여성질환 관련 검진 항목을 남성에 대해서는 미기입하는 경우
- **비무작위 결측** (MNAR, Missing Not At Random)
    - 결측 여부가 해당 변수의 실제 값과 직접적으로 관련된 경우
    - 예시: 소득이 매우 높은 사람이 소득을 의도적으로 응답하지 않음, 우울감이 높을수록 정신건강 관련 설문을 건너뛰는 경우

이런 결측치는 상황에 따라 삭제, 대체, 모델링하여 처리한다.


### 삭제

`palmerpenguins` 데이터셋을 이용하여 결측치를 삭제하는 방법을 알아 본다

```{python}
import pandas as pd
from palmerpenguins import load_penguins

df = load_penguins()
df.head()
```

컬럼별 결측치 개수를 확인한다.

```{python}
df.isna().sum()
```

```{python}
df.loc[df.isna().any(axis=1), :].reset_index()
```

결측치가 하나라도 있는 행은 삭제한다.

```{python}
df_dropna = df.dropna()
df_dropna.isna().sum()
```

### 대체

#### 수치형

컬럼이 수치형인 경우 평균값이나 중앙값 또는 특정값으로 대체한다.

```{python}
df_mean = df.copy()

num_cols = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]

na_index = df.loc[df.loc[:, num_cols].isna().any(axis=1), num_cols].index 
print(df_mean.loc[na_index, num_cols])

list_means = []
for col in num_cols:
    list_means.append(df_mean[col].mean())
    df_mean[col] = df_mean[col].fillna(df_mean[col].mean())

print(df_mean.loc[na_index, num_cols])
print(pd.Series(list_means, index=num_cols).reset_index())
```

각 컬럼이 범주로 분류된다면 다음과 같이 처리할 수 있다.

```{python}
df_mean = df.copy()

num_cols = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]

na_index = df.loc[df.loc[:, num_cols].isna().any(axis=1), num_cols].index 
print(df_mean.loc[na_index, ['species'] + num_cols])

for col in num_cols:
   df_mean[col] = df_mean.groupby(['species'])[col].transform(lambda x: x.mean())

print(df_mean.loc[na_index, ['species'] + num_cols])
```

#### 범주형

컬럼이 범주형인 경우 최빈값이나 특정 범주를 만들어 대체한다.

```{python}
import numpy as np

df_mode = df.copy()
num_cols = ["sex"]
df_mode.loc[277, 'sex'] = np.nan 

na_index = df_mode.loc[df_mode.loc[:, num_cols].isna().any(axis=1), num_cols].index 
print(df_mode.loc[na_index, ['species']+num_cols])
```

```{python}
df_mode[df_mode['species']=='Chinstrap']['sex'].value_counts()
```

```{python}
df_mode[df_mode['species']=='Chinstrap']['sex'].mode()
```

```{python}
list_modes = []
for col in num_cols:
    list_modes.append(df_mode[col].mode()[0])
    df_mode[col] = df_mode[col].fillna(df_mode[col].mode()[0])
    
print(df_mode.loc[na_index, ['species']+num_cols])
print(pd.Series(list_modes, index=num_cols).reset_index())
```

위 예제는 전체 컬럼 기준으로 최빈값을 계산 후 결측치를 대체한다. 아래 예제는 범주별 최빈값을 결측치에 대체하는 코드이다.

```{python}
list_modes = []
for col in num_cols:
    list_modes.append(df_mode[col].mode()[0])
    df_mode[col] = df_mode.groupby(['species'])[col].transform(lambda x: x.mode()[0])
    
print(df_mode.loc[na_index, ['species']+num_cols])
```


### 모델링

```{python}
import pandas as pd
from palmerpenguins import load_penguins
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegressor

# 1. 데이터셋 로드
penguins = load_penguins()

# 2. 수치형 데이터 선택
numeric_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_numeric = penguins[numeric_cols]

# 대체 전 결측치 확인
print("대체 전 결측치 상황:")
print(df_numeric.isnull().sum())
na_index = df_numeric.loc[df_numeric.isna().any(axis=1), :].index
print(df_numeric.loc[na_index, :])


# 3. IterativeImputer 설정
imputer = IterativeImputer(
    estimator=RandomForestRegressor(n_estimators=10, random_state=42),
    max_iter=10,
    random_state=42
)

# 4. 결측치 대체 수행
df_imputed = imputer.fit_transform(df_numeric)

# 5. 결과를 다시 데이터프레임으로 변환
df_final = pd.DataFrame(df_imputed, columns=numeric_cols)

print("\n대체 후 결측치 상황:")
print(df_final.isnull().sum())
print(df_final.loc[na_index, :])

# 원본 데이터와 합치기 (필요한 경우)
penguins[numeric_cols] = df_final

```



## 이상치 처리 


<br>
<pre>
마직막 편집일: 2025.12.28.
</pre>